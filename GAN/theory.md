# theory
## collapse
### GAN为什么容易训练崩溃
- 定义：所谓GAN的训练崩溃，指的是训练过程中，生成器和判别器存在一方压倒另一方的情况。
- 原因：
  - 判别器的loss达到最优的时候，生成器等价于最小化生成分布与真实分布之间的JS散度
  - 由于随机生成分布很难与真实分布有不可忽略的重叠，以及JS散度的突变特性，使得生成器面临梯度消失的问题
  - 可是如果不把判别器训练至最优，那么生成器优化的目标就失去了意义
  - 因此，我们需要小心地平衡两者，要把判别器训练得不好也不坏才行。否则就会出现训练崩溃，得不到想要的结果。
## WGAN
- 思想：提出使用Wasserstein距离，解决GAN训练过程中，难以判断收敛性的问题
- 公式：
- 损失函数：通过最小化Wasserstein距离，得到WGAN的loss
  - 生成器loss:
  - 判别器loss:
- 代码实现：
  - 判别器最后一层去掉sigmoid
  - 生成器和判别器的loss不取log
  - 每次更新判别器的参数后，把它们的绝对值截断到不超过一个固定常数c
## WGAN-GP
- 问题：实际实验发现，WGAN没有那么好用，原因在于WGAN进行了梯度截断。梯度截断将导致判别网络趋向于一个二值网络，造成模型容量的下降
- 解决方案：使用梯度惩罚来代替梯度截断
- 公式：
- 注意
  - 由于公式中是对每个梯度进行惩罚，所以不适合使用BN，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择Layer Normalization
  - 实际训练中，可以通过Wasserstein距离来度量模型收敛程度。一般Wasserstein距离趋于收敛，则生成图像也趋于稳定
## LSGAN
- 问题：标准GAN的稳定性不高，图片生成质量不高
- 解决方案：将原始GAN的交叉熵损失采用最小二乘损失替代
- 公式
- 原理：
  - 交叉熵损失sigmoid容易导致梯度饱和问题
- 实现：
  - 去掉最后一层sigmoid
  - 计算loss的时候用平方误差