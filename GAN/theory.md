# theory
## collapse
### GAN为什么容易训练崩溃
- 定义：所谓GAN的训练崩溃，指的是训练过程中，生成器和判别器存在一方压倒另一方的情况。
- 原因：
  - 直观解释：
    - 左边的正态分布曲线代表真实分布，右边的正态分布曲线代表生成器分布，两个分布没有重叠。中间那条竖线代表判别器，由于真实分布和生成器分布没有重叠，所以判别器能很好地将两个分布分开，从而导致梯度消失
  - 理论解释：
    - 判别器的loss达到最优的时候，生成器等价于最小化生成分布与真实分布之间的JS散度
    - 由于随机生成分布很难与真实分布有不可忽略的重叠，以及JS散度的突变特性，使得生成器面临梯度消失的问题
    - 可是如果不把判别器训练至最优，那么生成器优化的目标就失去了意义
    - 因此，我们需要小心地平衡两者，要把判别器训练得不好也不坏才行。否则就会出现训练崩溃，得不到想要的结果。
## WGAN: Wasserstein GAN
### 第一种GAN形式
- 损失函数
  - 判别器：-E_x~Pr(log(D(x))) - E_x~Pg(log(1 - D(x)))
  - 生成器：E_x~Pg(log(1 - D(x)))
- 推导
  - 当生成器固定时，最优判别器D*(x) = Pr(x) / Pr(x) + Pg(x)，是符合直观理解的，它等于一个样本x来自真实分布和生成分布的可能性的相对比例。如果Pr(x)=0且Pg(x)不等于0，最优判别器就应该非常自信地给出概率0；如果Pr(x)=Pg(x)，说明该样本是真是假的可能性刚好一半一半
  - 最优判别器下，生成器损失函数：2JS(Pr||Pg) - 2log2，即最小化真实分布Pr与生成分布Pg之间的JS散度。我们越训练判别器，它就越接近最优，最小化生成器的loss也就会越近似于最小化Pr和Pg之间的JS散度
  - 但问题就出在这个JS散度上（JS散度是一种衡量两个分布距离的指标）。我们本来是希望通过优化JS散度，来让Pg往Pr上靠，最终以假乱真。但这是建立在两个分布有所重叠的前提下。然而，当Pr和Pg没有重叠或重叠部分可忽略时，JS散度就是常数log2，梯度为0！
  - 那么Pr和Pg不重叠或重叠部分可忽略的可能性有多大呢？看下面的定理：当Pr与Pg的支撑集（support）是高维空间中的低维流形（manifold）时，Pr与Pg重叠部分测度（measure）为0的概率为1。
  - 我们来理解下这句话。 其中，支撑集就是函数的非零子集，比如ReLU的支撑集就是(0, 正无穷），Pr和Pg的支撑集就是概率密度不为零的地方。流形：三维空间中的一个曲面是一个二维流形，因为它的本质维度只有2。这里的高维空间（如果输出图片是64*64，就有4096维），如果输入z的维度为100维，则Pg撑死了也就100维。也就是说前半句是成立的。 测度：高维空间中长度、面积、体积概念的推广，可以理解成超体积。 
  - 所以第一种GAN形式的问题可归纳为：在（近似）最优判别器下，最小化生成器的loss等价于最小化Pr与Pg之间的JS散度，而由于Pr与Pg几乎不可能有不可忽略的重叠，所以无论它们相距多远JS散度都是常数log2，最终导致生成器的梯度（近似）为0，梯度消失。也正是因此，高分辨率图片生成更难，因为高维空间的维数更高
  - **原始GAN不稳定的原因就彻底清楚了：判别器训练得太好，生成器梯度消失，生成器loss降不下去；判别器训练得不好，生成器梯度不准，四处乱跑。只有判别器训练得不好不坏才行，但是这个火候又很难把握，所以GAN才那么难训练**
### 第二种GAN形式
- 损失函数
  - 判别器：-E_x~Pr(log(D(x))) - E_x~Pg(log(1 - D(x)))
  - 生成器：E_x~Pg(-log(D(x)))
- 推导
  - 最优判别器下，生成器损失函数：KL(Pg||Pr) - 2JS(Pr||Pg)
  - KL散度和JS散度都可以衡量两个分布之间的距离，前面一项是要把两个分布拉近，而后面一项却是要把两个分布推远。直观上很荒谬
  - 另外，KL散度有一个问题，它不是一个对称的衡量
    - 当Pg(x) -> 0，而Pr(x) -> 1时，对KL(Pg||Pr)贡献趋近0，对应情形：有一部分真实样本Pg没有覆盖到
    - 当Pg(x) -> 1，而Pr(x) -> 0时，对KL(Pg||Pr)贡献趋近正无穷，对应情形：Pg生成了一部分不真实的样本
  - 所以，**生成器会倾向于第一种情形，宁可生成一些重复但安全的样本，也不愿意去生成多样性的样本，因为那样一不小心就会产生第二种错误，得不偿失。这种现象就是大家常说的collapse**
### WGAN
- 思路：
  - 从上面的推导可以看出，不管是第一种形式的GAN，还是第二种形式的GAN，出现问题的重要原因就在于优化的目标距离不对，比如第一种GAN是优化JS散度，但两个分布没有重叠时，该散度为常数，不能提供梯度。在两个分布没有重叠时，不能反映分布距离的远近。因此，直观的思路就是不去优化JS散度，而是优化另外一种距离。这就是Wasserstein距离
  - 
- 思想：提出使用Wasserstein距离，解决GAN训练过程中，难以判断收敛性的问题
- 推导：
  - π（pr,pg）是指所有Pr和Pg联合分布的集合，对集合里的每个分布，都去算x和y的距离的期望，然后取上确界
  - 我们没法直接对该距离进行优化（由于在 GAN 中， Pr和 Pg 都没有显式的表达式，只能是从里面不停地采样，所以不可能找到这样的gamma，无法直接优化公式）。作者做了大量推导，得到Wasserstein距离的等价表示，推导要求函数f的Lipschitz常数小于K
  - 等价表示
    - 在要求函数f的Lipschitz常数||f||L不超过K的条件下，对所有可能满足条件的f取这个式子的上界，然后再除以K
    - 特别地，我们可以用一组参数w来定义一系列可能的函数fw，然后就得到这个式子。而fw就可以把用一个带参数的神经网络来表示嘛！由于神经网络的拟合能力足够强大，我们有理由相信，这样定义出来的一系列fw虽然无法囊括所有可能，但也能高度近似那个sup了
    - 为了满足||f||L不超过K这个限制，所以作者采取了一个非常简单的做法，就是限制神经网络fw的所有参数w都不超过某个范围[-c, c]，这样关于输入样本x的导数就不会超过某个范围，所以一定存在某个不知道的常数K使得fw的局部变动幅度不会超过它，Lipschitz连续条件得以满足。具体在算法实现中，只需要每次更新完w后把它clip回这个范围就可以了。也就是我们常用过的梯度裁剪
    - 所以我们的优化目标L即Wasserstein距离就成了Ex~Pr(fw(x)) - Ex~Pg(fw(x))，判别器要该距离尽可能大，而生成器要该距离尽可能小
- 损失函数：通过最小化Wasserstein距离，得到WGAN的loss
  - 生成器loss: Ex~Pg(fw(x)) - Ex~Pr(fw(x))
  - 判别器loss: -Ex~Pg(fw(x))
- 代码实现：
  - 判别器最后一层去掉sigmoid：原始GAN的判别器做的是真假二分类任务，所以最后一层是sigmoid，但是现在WGAN中的判别器fw做的是近似拟合Wasserstein距离，属于回归任务，所以要把最后一层的sigmoid拿掉
  - 生成器和判别器的loss不取log
  - 每次更新判别器的参数后，把它们的绝对值截断到不超过一个固定常数c
  - 不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行。这个是实践中发现的 
### Lipschitz连续
- 概念
  - Lipschitz连续，它其实就是在一个连续函数f上面额外施加了一个限制，要求存在一个常数K>=0,使得定义域内的任意两个元素x1和x2都满足式中的条件，此时称函数f的Lipschitz常数为K
  - 简单理解，比如说f的定义域是实数集合，那上面的要求就等价于f的导函数绝对值不超过K。比如sin(x)的Lipschitz常数为1
- 公式
### (dis)advantage
- 优点
  - 采用WGAN以后，**即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近，提供有效的梯度**，梯度接近线性
  - **Wasserstein距离代表真实分布和生成分布的距离**，而且在训练当中逐渐降低(也就是生成分布逐渐靠近真实分布，生成的图片也越来越逼真)，那么就可以**作为指示GAN训练是否收敛的指标**，而GAN的loss其实是没法做到这点的，这个不像是分类这样的任务，可以通过loss判断GAN是否收敛。 
- 缺点
## WGAN-GP
- 问题：实际实验发现，WGAN没有那么好用，原因在于WGAN进行了梯度截断。梯度截断将导致判别网络趋向于一个二值网络，造成模型容量的下降
- 解决方案：使用梯度惩罚来代替梯度截断
- 公式：
- 注意
  - 由于公式中是对每个梯度进行惩罚，所以不适合使用BN，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择Layer Normalization
  - 实际训练中，可以通过Wasserstein距离来度量模型收敛程度。一般Wasserstein距离趋于收敛，则生成图像也趋于稳定
## SNGAN
## LSGAN
- 问题：标准GAN的稳定性不高，图片生成质量不高
- 解决方案：将原始GAN的交叉熵损失采用最小二乘损失替代
- 公式
- 原理：
  - 交叉熵损失sigmoid容易导致梯度饱和问题
- 实现：
  - 去掉最后一层sigmoid
  - 计算loss的时候用平方误差
## problem
- KL散度和JS散度的理解，连续和离散形式
- 