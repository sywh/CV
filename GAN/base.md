# base
## conception
- GAN由生成器和判别器组成，生成器负责生成样本，判别器负责判断生成器生成的样本是否为真。生成器要尽可能迷惑判别器，而判别器要尽可能区分生成器生成的样本和真实样本
## objective function
- min max V(D, G) = min max E(logD(x)) + E(log(1-D(G(z))))
- 判别器的优化通过max V(D, G)实现。希望从真实数据分布中采样的样本，预测为正样本的概率接近1；生成器生成的样本，预测为正样本的概率接近0
- 生成器的优化通过min max(V(D, G))实现。注意，生成器的目标不是min V(D, G)，即生成器不是最小化判别器的目标函数，而是最小化判别器目标函数的最大值。判别器目标函数的最大值代表的是真实数据分布与生成数据分布的JS散度，JS散度可以度量分布的相似性，两个分布越接近，JS散度越小
## mode collapse
- 定义：某个模式(mode)出现大量重复样本，即生成样本缺乏多样性，存在大量重复。
- 解决方法：一个都没看懂哈哈哈？
  - 针对目标函数的改进方法：UnrolledGAN, DRAGAN, EBGAN
  - 针对网络结构的改进方法：MAD-GAN，MRGAN
  - Mini-batch Discrimination
## Discussion
### GAN的目标函数和交叉熵有什么区别
- 判别器的目标是最小化交叉熵损失，生成器的目标是最小化生成数据分布和真实数据分布的JS散度
### 为什么GAN的loss降不下去
- 作为一个训练良好的GAN，其loss就是降不下去的。衡量GAN是否训练好了，只能由人肉眼去看生成的图片质量是否好。
  - 由min max V(D, G)可以看出，生成器和判别器的目的相反，也就是说生成器网络和判别器网络互为对抗，此消彼长，不可能loss一直降到一个收敛的状态
  - 对生成器，其loss下降快，很有可能是判别器太弱，导致生成器很轻易的就愚弄了判别器
  - 对于判别器，其loss下降快，意味着判别器很强，判别器很强则说明生成器生成的图像不够逼真，才使得判别器轻易判别，导致loss下降很快。
  - 也就是说，无论是生成器还是判别器，loss的高低不能代表生成器的好坏。一个好的GAN网络，GAN的loss往往是不断波动的。
- WGAN：提出一种新的loss，较好地解决了难以判断收敛性的问题
### 生成式模型和判别式模型的区别
- 生成式模型：由数据学习联合概率分布P(X, Y), 然后由P(Y|X) = P(X, Y)/P(X)间接求出概率分布P(Y|X)作为预测结果。生成模型直接拟合P(X, Y)，比如用二维高斯分布来拟合数据。
- 判别式模型：由数据直接学习决策函数Y=f(X)或条件概率分布P(Y|X)作为预测模型。判别方法关心的是对于给定的输入X，应该预测什么样的输出Y。
- 判别模型需要的数据较少，因为判别式模型更光柱输入特征的差异性，而生成式模型要学习联合分布，往往需要更多的数据。不过生成式模型既然使用了更多数据来生成联合分布，自然能提供更多的信息。例如，现在有一个样本(X, Y)，其联合概率P(X, Y)经过计算特别小，那么可以认为这个样本是异常样本，这种模型可以用来做outlier detection
## problem
- 为什么博弈的最终结果是，生成器生成的图像使得判别器完全无法判别真假，而不是判别器能很好地识别出生成器生成的图像呢？