# GAN
## CGAN
### base
- 作用
  - 原始GAN只能对整个数据分布进行采样，无法对数据分布中某种模式进行采样，比如对指定类别的数据进行采样，而条件GAN通过将条件信息嵌入到生成器和判别器中，可以实现条件采样
  - 一定程度上解决GAN生成结果的不确定性
- 例子：mnist
  - 原始GAN生成的图像是完全不确定的，具体生成的数字是几，完全不可控
  - 简单方法：为了让生成的数字可控，可以把数据集做一个切分，把数组0-9的数据集分别拆开训练训练9个模型，但这不仅分类麻烦，更重要的是，每一个类别的样本少，拿去训练GAN很可能导致欠拟合
  - 结构：
    - Generator：在基本的GAN模型中，生成器的输入是满足某个分布的随机噪声；在CGAN中，不仅要输入随机噪声z，还需要将之与标签类别y做拼接(concat，一般要将标签转换成如one-hot或者做embedding)，再将其输入生成器生成所需要的数据
    - Discriminator：在基本的GAN模型中，判别器的输入是真实数据或生成数据；在CGAN中，需要将真实数据或生成数据跟对应的标签类别先做拼接，再输入判别器的神经网络进行识别和判断
### loss
- 公式：
- 理解：
  - loss设计和原始GAN基本一致
  - 只不过生成器和判别器的输入数据是一个条件分布
  - 具体编程实现时，只需要对随机噪声采样z和输入条件y做一个级联即可
## DCGAN
### base
- 引入
  - 原始GAN基于多层感知机实现，调参难度大，生成图片质量不佳。DCGAN利用CNN代替MLP，并采用BN和LeakeyReLU等一系列的trick，得到了效果很好的卧室照片和在当时效果还不错的人脸图片
- 问题
  - 对于视觉问题，如果使用基于DNN的GAN，则整个模型参数会非常巨大
  - 学习难度很大（低维度映射到高维度需要添加许多信息）
- 解决方案
  - DCGAN：将传统GAN的生成器和判别器均采用CNN实现
  - 使用了很多tricks
### tricks
- 使用CNN代替MLP
- 使用convolution代替pooling。其中，在discriminator上用strided convolution替代，在generator上用fractional-strided convolution替代
- 在generator和discriminator上都使用batchnorm
- 用global pooling代替全连接层，增加了模型稳定性，但损害了收敛速度
- generator输出层激活函数采用tanh，其它层使用relu
- disciminator所有层使用LeakeyReLU：**训GAN的时候容易发生梯度弥散，因此采用LeakeyReLU代替ReLU是GAN网络的标配**
### exp
- 了解输入随机噪声每一个维度代表的含义
  - 在隐空间上，假设知道哪几个变量控制着某个物体，那么将这几个变量挡住是不是就可以将生成图片中的某个物体消失？
  - 实验
    - 首先，生成150张图片，包括有窗户的和没窗户的
    - 然后，使用一个逻辑斯蒂回归函数来进行分类
    - 对于权重不为0的特征，认为它和窗户有关，将其挡住（如何将其挡住?），得到新的生成图片
  - 算数运算
    - 将几个输入噪声进行算术预案算，可以得到语义上进行算术运算的非常有趣的结果
    - 类似于word2vec
### dis(advantage)
- 优点
  - 极大提高了GAN生成数据的质量，由于DCGAN图片生成效果比较好，证明了用GAN生成图片的可行性，因此吸引了更多的研究者关注GAN
  - DCGAN还支持输入随机向量的插值和向量算术操作（这个操作是在输入的随机噪声上进行的）。比如下面左图，显示了最左边的卧室图片到最右边卧室图片的插值结果，右图则显示了生成人脸图片的向量算术操作(最左边是戴眼镜的男人的照片，然后是不戴眼镜的男人的照片，两者相减，然后加上不戴眼镜的女人照片，就得到了戴眼镜的女人照片。这跟NLP里面word embedding的算术操作类似)
- 缺点
  - 只能输出**64×64**分辨率的图片
## PGGAN
- 引入
  - DCGAN只能输出64×64分辨率的图片，这显然是不够的，经过一系列发展，在17年的时候，GAN输出图像分辨率主要在64×64到256×256之间
  - 生成高分辨率图像困难之处在于，**图片维度高，判别器很容易就能把生成的图片判别成假的，这样生成器就会发生梯度弥散，GAN就训不下去了**
- 思路
  - PGGAN提出了一种生成器和判别器逐层增长的训练方法，从低分辨率(4×4)图像逐步生成高清图(1024×1024)
- 做法
  - 先在4×4分辨率下训GAN，训好以后，再加8×8的layer再训，直到1024
### dis(advantage)
- 优点
  - PGGAN首次实现了**1024×1024**分辨率人脸的生成，以及LSUN数据集上256×256分辨率图片的生成
  - 提供了一种生成高分辨率图片的范式
- 缺点
  - PGGAN由于要训多组GAN，训练速度比较慢
  - 没有攻克在ImageNet这样的复杂数据集上生成高分辨率图片的难题
## BigGAN
- 引入
  - 丰富的背景和纹理图像的生成是各类生成模型追求的终极目标，在GAN中，ImageNet的生成已然成为检验生成模型好坏的一个指标
- 做法
  - BigGAN通过采用大规模的GAN，以及**截断技巧**等方式，把ImageNet数据生成的点刷的很高
- 效果
  - 128*128分辨率下把Inception score从52.52刷到了166.5，FID从18.65刷到了7.4
## StyleGAN
## StyleGANv2
## problem
- cGAN中条件是如何加入到生成器尤其是判别器的？
- cGAN中的loss是什么样的？网络如何能考虑到条件信息？为什么不用判别器分类做cross entropy？
- 为什么判别器使用LeakeyReLU？为什么生成器不适用LeakeyReLU？训GAN的时候为什么容易发生梯度弥散？是生成器梯度弥散还是判别器梯度弥散？