# hyperparameters
## conception
- 模型容量：各种超参数都会直接或间接影响网络模型的有效容量
- 与参数区别：参数通常通过学习算法，由数据来驱动更新，即在反向传播中更新；而超参数则是人为进行调整的参数
- 超参数分类
  - 网络参数：如网络层与层之间交互方式（相加、相乘或者拼接等）、卷积核数量和尺寸、网络层数、激活函数等
  - 优化参数：如学习率，batch size，优化器的参数，损失函数的可调参数等
  - 正则化：权重衰减系数，dropout比例
- 超参数调优的本质：平衡优化寻找最优解和正则项之间的关系，最小化期望风险
- 超参数的重要性排序
  - 学习率，损失函数上的可调参数
  - 批样本数量，动量优化器的动量参数beta(推荐参数0.9)
  - Adam优化器的超参数、权重衰减系数、dropout比例、网络参数：实践中不建议过多尝试。
- 超参数推荐值
  - 学习率
    - SGD：推荐参数[1e-2, 1e-1]
    - momentum：推荐参数[1e-3, 1e-2]
    - Adam：推荐参数[1e-3, 1e-2]
    - 微调：初始学习率可降低1-2个数量级
  - 损失函数上的可调参数
    - 多个损失函数之间，损失值尽可能相近，不建议超过或者低于两个数量级
  - 批样本数量：[1, 1024]
  - 优化器参数
    - Adam优化器参数beta1, beta2, eposilon，推荐值0.9, 0.999, 1e-8
  - 正则化参数
    - 权重衰减系数：推荐值0.0005, [0, 1e-4]?
    - dropout比例：推荐值[0.2, 0.5]
  - 网络参数
    - 卷积核尺寸：[7*7], [5*5], [3*3], [1*1], [7*1, 1*7]
## learning rate
- 过高或过低的学习率，都会降低模型的有效容量
- 学习率最优点，在训练的不同时间点都可能变化，所以需要一套有效的学习率衰减策略。另外，模型可能陷入局部极小值或者鞍点。
- 常见的学习率衰减方式
  - 指数衰减：在训练初期衰减较大，利于收敛；在后续衰减较小，利于精调；
  - 分段衰减：利于精调
  - 多项式衰减
  - 逆时衰减
  - 余弦衰减
  - 余弦衰减+重启：周期重启
  - 线性余弦衰减：主要应用于增强学习领域
  - 噪声线性余弦衰减
## batch size
- batch size 小
  - 梯度不稳定：每次梯度更新的方向不能很好地代表样本总体，可能相互抵消，难以达到收敛
    - 应对策略：采用累计梯度策略。每次计算完不反向封信，而是累计多次的误差后进行依次更新，这是一种在内存有限情况下实现有效梯度更新的策略
  - batch norm统计不准确
    - 应对策略（采用预训练模型）：冻结预训练网络中batchnorm的模型参数，有效降低batchsize引起的统计量变化的影响(都冻结吗?)
    - 应对策略（网络不是过深）：直接移除batchnorm或者使用groupnorm代替batchnorm
- batch size 大
  - 优点
    - 内存利用率提高
    - 大矩阵乘法的并行化效率提高
    - 梯度下降方向较准，引起训练震荡较小
  - 缺点
    - 内存容量限制
    - 一个epoch内迭代次数少，对参数的修正缓慢，要想达到相同的精度，需要的epoch数量增加吧。
    - 梯度下降的方向基本不再变化
  - 应对策略
    - 层自适应速率缩放(LARS)算法：batchsize增大会减少反向传播的梯度更新次数，为了达到相同的效果，需要增大学习率。但学习率一旦增大，又会引起模型的不收敛。为了解决该矛盾，LARS算法在各层上自适应地计算一个本地学习率用于更新本层的参数，这样能有效提升训练的稳定性。
- trade off
  - batch size大，并行化效率高，但需要的epoch数量增加。因此，当batch size增大到某个时候，达到时间上的最优
  - batch size大，梯度下降方向较准，但会陷入局部极值。因此，当batch size增大到某个时候，达到收敛精度上的最优
- 推荐大小：[1, 1024]
## 预训练网络
### 微调方法
- 不冻结网络模型的任何层：对最后的改动层使用较大的学习率，对未改动层使用较小的学习率。一步完成训练。
- 冻结除了顶层改动层以外的所有层参数：进行若干轮微调训练后，放开顶部层以下的若干层或者全部放开所有层的参数，再次进行若干轮训练。分多步训练。
- 可以先通过第一种方式找到适合的预训练模型，然后通过第二种方式进行精调。
### 先冻结底层，训练顶层的原因
- 防止顶层糟糕的初始化，对已经具备一定表达能力的层的干扰和破坏，影响最终的性能
- 越底层的特征通常是越通用的特征，越往上其整体的高层次语义越玩呗，这通过感受野很容易理解。因此，若预训练模型和微调模型的数据语义差异越大，那越往顶层的特征语义差异就越大，因此通常需要进行相应的调整
### 不同数据集特征如何微调
- 数据集小，和原数据集类似：只修改最后的输出层，进行训练。训练过多参数容易过拟合
- 数据集小，和原数据集差异较大：在完成输出顶层的微调后，放开顶部层往下一半的层数
- 数据集大，和原数据集类似：放开所有层，以较小的学习率微调
- 数据集大，和原数据集差异较大：不需要用预训练模型进行微调，直接重新训练
### 目标检测中使用预训练模型
- 优势
  - 目标检测中one-stage和two-stage方法都基于ImageNet上预训练好的分类模型，预训练模型已拥有优秀的语义特征，能有效加快训练速度
  - two-stage方法并未实现严格意义上的完全端到端训练，所以使用预训练模型能直接提取到语义特征，使两个阶段的网络更容易优化
- 劣势
  - 分类模型和检测模型存在任务上的差异。分类模型大部分训练于单目标数据，对同时进行多目标的捕捉能力较弱，且不关注目标的位置，在一定程度上让模型损失部分空间信息，这对检测模型通常是不利的
  - 域适应问题：若预训练模型(ImageNet)和实际检测器的使用场景(医学图像、卫星图像)差异较大时，性能会受到影响
  - 限制应用场合：使用预训练模型就意味着难以自由改变网络结构和参数
### 目标检测从良开始训练
- 根据FAIR研究，只要拥有足够的数据以及充分有效的训练，同样能训练处不亚于利用预训练模型的检测器
- 训练建议
  - 数据集不大时，需要进行数据增强
  - train from scratch需要更多的迭代次数和时间，来训练和优化检测器。二姐模型由于不是严格的端到端训练，可能需要更多的迭代次数和时间。
  - train from scratch最大的问题是batch size过小。可采取的策略包括
    - 增加GPU使用异步batchnorm增大batch size
    - 若无法使用更多GPU，可使用groupnorm代替batchnorm
  - 由于分类模型存在对多目标捕捉能力若，以及对物体空间位置信息不敏感等问题，可借鉴DetNet训练一个专属于目标检测的模型，增强对多目标、尺度和位置的适应性
## 优化GAN的性能
### 方向
- 设计或选择更适合目的的代价函数
- 添加额外的惩罚？
- 避免判别器过度自信和生成器过度拟合
- 更好的优化模型的方法
- 添加标签，明确优化目标
### 训练技巧
- 将输入规范化到(-1, 1)之间，最后一层激活函数使用tanh(BEGAN除外)
- 使用wassertein GAN损失函数
- 标签：
  - 有标签数据的话，尽量使用标签，也有人提出使用反转标签效果很好
  - 使用标签平滑，单边标签平滑或者双边标签平滑
- 使用mini-batch norm，如果不用batch norm，可以使用instance norm或weight norm
- 避免使用relu和pooling层，减少稀疏梯度的可能性，可以使用leakeyRelu
- 优化器尽量选择Adam，学习率不要设置太大，初始学习率可参考1e-4，另外可以随着训练不断衰减学习率
- 对D的网络层增加高斯噪声，相当于是一种正则
## AutoML
不了解，暂时先不写了
## reference
- github: https://github.com/scutan90/DeepLearning-500-questions