# Normalization
## Base
### 归一化定义
### 归一化作用
- 在神经网络中，对激活函数的输入进行Norm

## Batch Normalization
- 定义：
  - 在batch上，对NHW做归一化
  - 对各个通道，计算该通道上的均值和方差，然后做归一化
  - 使数据保持均值为0，方差为1的分布，避免梯度消失
- 做法：
  - 沿着通道计算每个batch的均值和方差，做归一化
  - 加入缩放和平移变量
- 公式：
- 区分：
  - 不同的channel有不同的均值和方差
- 为什么要做BN：
  - 不仅对输入层数据进行归一化处理，对中间层数据也进行归一化处理
  - 每个batch具有不同的分布，使模型训练起来特别困难
  - Internal Covariate Shift(ICS)问题：训练过程中，激活函数会改变各层数据的分布，随着网络的加深，这种改变会越来越大，使模型训练起来特别困难，收敛速度慢，出现梯度消失
- 为什么要加入缩放和平移变量：
  - 保证数据经过归一化后还保留原有学习来的特征，同时又能完成归一化操作，加速训练
- 使用位置
  - 全连接层或卷积操作之后，激活函数之前
- 适用范围
  - 较大的batch size：BN对较小的batch size效果不好。在训练之前，要做好充分的shuffle，否则效果会差很多。
  - 固定深度的网络：BN适用于固定深度的前向神经网络，如CNN，不适用于RNN：
- BN的作用
  - 允许较大的学习率
  - 减弱对初始化的强依赖性
  - 保持隐藏层中数值的均值、方差不变，让数值更稳定
  - 有轻微的正则化作用（相当于给隐藏层加入噪声？类似Dropout）
- BN存在的问题
  - batch size太小: 计算得到的均值和方差不足以代表整个数据分布
  - batch size太大: 会超出内存容量；会沿着固定梯度下降的方向，导致很难更新?需要跑更多的epoch，导致总训练时间变长
- 不理解的地方
  - 为什么通道不做归一化？
  - ICS问题怎么理解？
  - 为什么要加入缩放和平移?
  - 为什么使用位置有限制？
  - 为什么BN具有那些作用？
  - 为什么batch size太大反而梯度更新不好？
## Layer Normalization
- 定义：在channel上，对CHW做归一化
- 做法：
  - 对每个样本，计算所有channel的均值和方差，做归一化
- 区分：
  - 不同的输入样本有不同的均值和方差
- 公式：
- 适用范围
  - RNN：深度不固定的网络，sequence长度不一致
- LN的优点
  - 不需要批训练：在单条数据内部就能归一化，可以用于batch size为1中
  - 不依赖输入sequence长度：用于RNN效果比较明显
- LN的缺点
  - 在CNN上，效果不如BN
## Instance Normalization
- 定义：在pixel上，对HW做归一化
- 做法：
  - 对每个样本，每个channel，求均值和方差，做归一化
- 公式：
  - 
- 适用范围
  - 风格迁移：生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中；另外，feature map的各个channel的均值和方差会影响到最终生成图像的风格，因此对H、W做归一化
- IN的优点：
  - 可以加速模型收敛
  - 保持每个图像实例之间的独立
## Group Normalization
- 定义：将channel分组，然后再做归一化
- 做法：在channel方向分group，对每个group做norm，计算(C/G)*H*W的均值和方差
- 区分：
  - 介于LN和IN之间，每组channel用其对应的归一化参数独立地归一化
- 适用范围：
  - 占用显存比较大的任务，如图像分割
- GN的优点
  - 不需要批训练

## reference
- 文章：常用的 Normalization 方法：BN、LN、IN、GN https://cloud.tencent.com/developer/article/1517151