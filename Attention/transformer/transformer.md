# transformer
## base
### RNN
- base
    - RNNs is a typical network architecture for translation, processing language **sequentially** in a left-to-right or right-to-left fasion
    - Reading one word at a time, this forces RNNs to **perform multiple steps to make decisions** that depend on words far away from each other.
    - Processing the example above, an RNN could only determine that “bank” is likely to refer to the bank of a river after reading each word between “bank” and “river” step by step
    - roughly speaking, the more such steps decisions require, the harder it is for a recurrent network to learn how to make those decisions.
    - The sequential nature of RNNs also makes it more difficult to fully take advantage of modern fast computing devices such as TPUs and GPUs, which excel at parallel and not sequential processing.
    - Convolutional neural networks (CNNs) are much less sequential than RNNs, but in CNN architectures like ByteNet or ConvS2S the number of steps required to combine information from distant parts of the input still grows with increasing distance.
- disadvantage
  - process language sequentially -> preform multiple steps to make decisions
  - TPUs and GPUs is excel at parallel and not sequential processing.
- conclusion
  - RNN是序列结构，串行预测，如果两个单词相隔很远，则需要多步来进行决策
  - 多步决策不仅会影响效果，也不适用于GPUs和TPUs等擅长并行计算的设备
## transformer
### overview
- In contrast, the Transformer only performs a small, constant number of steps (chosen empirically). In each step, **it applies a self-attention mechanism which directly models relationships between all words in a sentence, regardless of their respective position.**
- In the earlier example “I arrived at the bank after crossing the river”, to determine that the word “bank” refers to the shore of a river and not a financial institution, **the Transformer can learn to immediately attend to the word “river” and make this decision in a single step.**
- More specifically, to compute the **next representation** for a given word - “bank” for example - the Transformer compares it to every other word in the sentence. The result of these comparisons is an attention score for every other word in the sentence. These attention scores determine how much each of the other words should contribute to the next representation of “bank”
- In the example, the disambiguating “river” could receive a high attention score when computing a new representation for “bank”.
- The attention scores are then used as weights for a weighted average of all words’ representations which is fed into a fully-connected network to generate a new representation for “bank”, reflecting that the sentence is talking about a river bank.
- The Transformer starts by generating initial representations, or embeddings, for each word. These are represented by the unfilled circles. Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls.
- This step is then repeated multiple times in parallel for all words, successively generating new representations.
- The decoder operates similarly, but generates one word at a time, from left to right. It attends not only to the other previously generated words, but also to the final representations generated by the encoder.
- Beyond computational performance and higher accuracy, another intriguing aspect of the Transformer is that we can visualize what other parts of a sentence the network attends to when processing or translating a given word, thus gaining insights into how information travels through the network.
- conclusion
  - Transformer通过self-attention直接建模所有单词的关系，不管单词间相隔多远，因此可以一步就进行决策（确定单词多种含义中的一种）
  - self-attention先比较bank和句中其它单词，得到attention score，这些socre代表其它单词对bank的新表示的权重
  - 通过attention scores得到所有单词的加权表示，然后传入全连接网络，得到bank的新表示, 新表示能反映出句子在讨论的是河岸
  - Transformer先对每个单词做embedding，然后用self-attention来聚集所有单词的信息，得到新的表示，新表示包含完整的上下文信息。通过对每个单词多次并行执行self-attention，可以不断得到新的表示
  - 解码时，每次从左往右生成一个单词。解码attention包括encoder中每个单词的最终表示，以及解码生成的前面的单词的表示
### detail
- 网络结构
  - 编码组件：6层编码器堆叠而成，编码器结构完全相同，但不共享参数
    - self-attention层：编码某个单词时，帮助编码器能够看到输入序列中的其它单词
    - 前向网络：每个输入位置对应的前向网络是独立互不干扰的
  - 解码组件：6层解码器堆叠而成
    - self-attention层
    - Encoder-Decoder attention层：帮助解码器能够关注到输入句子的相关部分，与seq2seq model的attention作用相似
    - 前向网络
- 编码器
  - embedding：先对输入单词使用embedding算法转成512维的词向量，所有向量组成list，list的尺寸通常是训练集的最长句子的长度
  - self-attention
    - 当模型处理每个位置的词时，self-attention允许模型看到句子的其它位置信息作为辅助线索，来更好地编码当前词
    - 1. 对编码器的输入向量，生成3个向量：query, key和value。生成方法为分别乘以3个矩阵，这些矩阵在训练过程中学习。
      - 注意，不是每个词向量独享3个matrix，而是所有输入共享3个转换矩阵。**权重矩阵是基于输入位置的转换矩阵（？）**
      - 新向量的维度比输入词向量的维度要小(512->64)，并不是必须要小，是为了让多头attention的计算更稳定
      - **query/key/value是什么？**对计算和思考attention是有益的
    - 2. 计算attention：attention分值决定着编码某个单词时，每个输入词需要集中多少关注度。分值通过某单词对应的query和所有词的key依次做点积得到。所以，当我们处理位置#1时，第一个分值是q1和k1的点积，第二个分值是q1和k2的点积
    - 3. 除以sqrt(dim_key)：这样梯度会更稳定（为什么？）
    - 4. softmax：归一化分值使得全为正数且加和为1。softmax分值决定在这个位置，每个词的关注度。很明显，这个位置的词应该有最高的归一化分数，但大部分时候总是有助于关注该词的相关的词
    - 5. 将softmax分值与value按位相乘：保留关注词的value值，削弱非相关词的value值
    - 6. 向量加权，产生该位置的self-attention的输出结果
    - 矩阵形式：
      - 1. 计算query/key/value matrix：将所有输入词向量(x1, x2等)合并成输入矩阵X，并将其分别乘以权重矩阵W_q, W_k，W_v
      - 2. 鉴于我们使用矩阵处理，将步骤2~6合并成一个计算self-attention层输出的公式 Z = softmax(Q*KT / sqrt(dim_key))*V
  - multi-heads
    - 优点
      - 多头机制扩展了模型集中于不同位置的能力。上面例子中，z1值包含了其它词的很少信息
      - 多头机制赋予attention多种子表达方式。多头下有**多组query/key/value**，而非仅仅一组。每一组都是随机初始化，经过训练之后，输入向量可以被映射到不同的子表达空间中
    - 做法
      - 8组权重矩阵，对应8组Q/K/V矩阵，得到8个attention heads(Z1,...,Z8)
      - 由于前向网络输入不能接受8个矩阵，希望输入是一个矩阵，故将8个attention heads拼接，然后和权重矩阵W0相乘，得到最终attention矩阵Z，Z捕捉了所有attention heads的信息
  - Positional Encoding
    - 理解输入词句中词的顺序
    - 为了解决词序的利用问题，transformer新增了一个向量，编码词的位置
    - 编码方法：
      - 对每个单词，编码一个512维的位置向量，分别通过sin和cos函数生成
  - The Residuals
    - 在每个子层(self-attention, ffnn)中，都有残差连接，并且紧跟着layer normalization
  - 特性
    - 每个位置的词仅仅流过它自己的编码器路径。在self-attention层中，这些路径两两之间是相互依赖的。前向网络层则没有这些依赖性，但这些路径在流经前向网络时可以并行执行
- 解码器
  - 编码器的输出被转换为K和V，被每个解码器的encoder-decoedr attention层使用，帮助解码器集中于输入序列的合适位置
  - 每一步的输出被喂到下一个解码器中，直到一个特殊符号出现表示解码器完成了翻译输出
  - 如编码器的输入所做的处理，对解码器的输入增加位置向量
  - self-attention层：与编码器稍有不同，解码器中，self-attention层仅允许关注早于当前输出的位置，在softmax之前，通过遮挡未来位置(将它们设置为-inf)来实现。
  - encoder-decoder attention层：和multi-head self-attention一样。除了一点，它从前层获取输出转成query矩阵，接收最后层编码器的key和value矩阵做key和value矩阵
- 问题：
  - 为什么要加入layer normalization？
  - multi-head有什么作用？不会学的东西一样吗？
  - 解码器self-attention为什么仅关注早前输出位置？
  - encoder-decoder attention层为什么取编码器的key和value矩阵？
  - 在视觉中，为什么CNN是有效的，CNN有哪些假设？为什么Transformer可以应用到视觉中？CNN vs transformer的优劣势？为什么transformer需要更多的数据量来训练？
- 输出层
  - 线性层：简单的全连接层，加解码器的最后输出映射到一个非常大的logits向量上，向量维数和此表大小一致，每个值表示是某个词的可能倾向值
  - softmax层：将分数转换为概率值，最高值对应的词就是这一步上的输出单词
- 损失函数
  - 对单个单词来说，就是softmax
  - 对sentence来说，是多个softmax之和吗？
- 预测：
  - greedy解码：每步选择最高概率对应的词
  - beam search解码：每步保留最高的两个概率对应的词，根据这两个词再预测下一步，再保留最高的两个概率对应的词
## advantage
- higher translation quality
- less computation to train
- **parallel**, is a much better fit for modern machine learning hardware, speeding up training by up to an order of magnitude
- better to visualize
## disadvantage
## reference
- **Google AI Blog**: Transformer: A Novel Neural Network Architecture for Language Understanding https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html
- 技术博客：http://jalammar.github.io/illustrated-transformer/ 及其中文翻译 https://blog.csdn.net/yujianmin1990/article/details/85221271