# overview
## base
- 本质
  - 从关注全部到关注重点
  - 将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息
  - 类似人类看图片的逻辑：当我们看一张图片的时候，我们并没有看清图片的全部内容，而是将注意力集中在了图片的焦点上
- 引入attention机制原因
  - 根据通用近似定理，前馈网络和循环网络都有很强的能力。但为什么还要引入注意力机制呢
    - 计算能力的限制：当要记住很多"信息"，模型就要变得更复杂，然而目前计算能力依然是限制神经网络发展的瓶颈
    - 优化算法的限制：虽然局部连接、权重共享以及pooling等优化操作可以让神经网络变得简单一些，有效缓解模型复杂度和表达能力之间的矛盾；但是，如循环神经网络中的长距离依赖问题，信息“记忆”能力并不高
  - 可以借助人脑处理信息过载的方式，例如Attention机制可以提高神经网络处理信息的能力
  - 长距离依赖问题
    - 卷积或循环神经网络不能很好地处理长距离序列。当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列。无论卷积还是循环神经网络其实都是对变长序列的一种“局部编码”：卷积神经网络显然是基于N-gram的局部编码；而对于循环神经网络，由于梯度消失等问题也只能建立短距离依赖。
    - 如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法
      - 一种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互
      - 另一种方法是使用全连接网络，全连接网络虽然是一种非常直接的建模远距离依赖的模型， 但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的
      - 这时我们就可以利用注意力机制来“动态”地生成不同连接的权重，这就是自注意力模型（self-attention model）。由于自注意力模型的权重是动态生成的，因此可以处理变长的信息序列。**self-Attention中的Q是对自身（self）输入的变换，而在传统的Attention中，Q来自于外部**
- 流程
  - Attention机制的实质其实就是一个寻址（addressing）的过程，如上图所示：给定一个和任务相关的查询Query向量 q，通过计算与Key的注意力分布并附加在Value上，从而计算Attention Value，**这个过程实际上是Attention机制缓解神经网络模型复杂度的体现：不需要将所有的N个输入信息都输入到神经网络进行计算，只需要从X中选择一些和任务相关的信息输入给神经网络**
  - 注意力机制可分为三步
    - 信息输入
    - 计算注意力分布α
    - 据注意力分布α来计算输入信息的加权平均
## (dis)advantage
- 优点
  - 参数少：模型复杂度跟CNN、RNN相比，复杂度更小，参数也更少。所以对算力的要求也就更小
  - 速度快：RNN不能并行计算，而Attention机制每一步计算不依赖于上一步的计算结果，因此可以和CNN一样并行处理
  - 效果好：
    - Attention机制引入之前，有一个问题：长距离的信息会被弱化，就好像记忆能力弱的人，记不住过去的事情是一样的
    - Attention是挑重点，就算文本比较长，也能从中间抓住重点，不丢失重要的信息。即，attention让模型懂得了提纲挈领
- 缺点
## category
- 计算区域
  - Soft Attention: 比较常见的attention方式，对所有key求权重概率，每个key都有一个对应的权重，是一种全局的计算方式（也可以叫Global Attention）。这种方式比较理性，**参考了所有key的内容，再进行加权**。但是计算量可能会比较大一些
  - Hard Attention: 这种方式是直接精准定位到某个key，其余key就都不管了，相当于这个key的概率是1，其余key的概率全部是0。因此这种对齐方式要求很高，要求一步到位，如果没有正确对齐，会带来很大的影响。另一方面，因为不可导(基于最大采样的方式选择信息，最终的损失函数与注意力分布之间的函数关系不可导，因此无法使用反向传播算法进行训练)，一般需要用强化学习的方法进行训练（或者使用gumbel softmax之类的）
  - Local Attention: 这种方式其实是以上两种方式的一个折中，对一个窗口区域进行计算。先用Hard方式定位到某个地方，以这个点为中心可以得到一个窗口区域，在这个小区域内用Soft方式来算Attention（这种方式不会有不可导的问题吗？）
- 所用信息
  - 假设我们要对一段原文计算Attention，这里原文指的是我们要做attention的文本，那么所用信息包括内部信息和外部信息，内部信息指的是原文本身的信息，而外部信息指的是除原文以外的额外信息。
  - 没太看懂
- 结构层次
  - 结构方面根据是否划分层次关系，分为单层attention，多层attention和多头attention
  - 单层attention: 这是比较普遍的做法，用一个query对一段原文进行一次attention
  - 多层attention: 一般用于文本具有层次关系的模型，假设我们把一个document划分成多个句子，在第一层，我们分别对每个句子使用attention计算出一个句向量（也就是单层attention）；在第二层，我们对所有句向量再做attention计算出一个文档向量（也是一个单层attention），最后再用这个文档向量去做任务。
  - 多头attention: 这是Attention is All You Need中提到的multi-head attention，用到了多个query对一段原文进行了多次attention，每个query都关注到原文的不同部分，相当于重复做多次单层attention，最后再把这些结果拼接起来
- 模型方面
  - CNN + Attention
    - CNN的卷积操作可以提取重要特征，我觉得这也算是Attention的思想，但是CNN的卷积感受视野是局部的，需要通过叠加多层卷积区去扩大视野
    - 另外，Max Pooling直接提取数值最大的特征，也像是hard attention的思想，直接选中某个特征
    - CNN上加Attention可以加在这几方面
      - 在卷积操作前做attention，比如Attention-Based BCNN-1，这个任务是文本蕴含任务需要处理两段文本，同时对两段输入的序列向量进行attention，计算出特征向量，再拼接到原始向量中，作为卷积层的输入
      - 在卷积操作后做attention，比如Attention-Based BCNN-2，对两段文本的卷积层的输出做attention，作为pooling层的输入
      - 在pooling层做attention，代替max pooling。比如Attention pooling，首先我们用LSTM学到一个比较好的句向量，作为query，然后用CNN先学习到一个特征矩阵作为key，再用query对key产生权重，进行attention，得到最后的句向量
  - LSTM + Attention
    - LSTM内部有Gate机制，其中input gate选择哪些当前信息进行输入，forget gate选择遗忘哪些过去信息，我觉得这算是一定程度的Attention了，而且号称可以解决长期依赖问题
    - 实际上LSTM需要一步一步去捕捉序列信息，在长文本上的表现是会随着step增加而慢慢衰减，难以保留全部的有用信息
    - LSTM通常需要得到一个向量，再去做任务，常用方式有
      - 直接使用最后的hidden state（可能会损失一定的前文信息，难以表达全文）
      - 对所有step下的hidden state进行等权平均（对所有step一视同仁）
      - Attention机制，对所有step的hidden state进行加权，把注意力集中到整段文本中比较重要的hidden state信息。性能比前面两种要好一点，而且方便可视化观察哪些step是重要的，但是要小心过拟合，而且也增加了计算量
  - 纯Attention
    - Attention is all you need，没有用到CNN/RNN，乍一听也是一股清流了，但是仔细一看，本质上还是一堆向量去计算attention
- 相似度计算方式：在做attention的时候，我们需要计算query和某个key的分数（相似度），常用方法有
  - 点乘：最简单的方法
  - 矩阵相乘
  - cos相似度
  - 串联方式：把q和k拼接起来
  - 用多层感知机
- 认知神经学
  - 聚焦式（focus）注意力：自上而下的有意识的注意力，主动注意——是指有预定目的、依赖任务的、主动有意识地聚焦于某一对象的注意力。在人工神经网络中，注意力机制一般就特指聚焦式注意力
  - 显著性（saliency-based）注意力：自下而上的有意识的注意力，被动注意——基于显著性的注意力是由外界刺激驱动的注意，不需要主动干预，也和任务无关；可以将max-pooling和门控（gating）机制来近似地看作是自下而上的基于显著性的注意力机制
## problem
- Attention能并行计算的原因是什么？RNN不能并行计算，实际是怎么实现的？
- 由hard attention引出，什么时候可导，什么时候不可导？
- self-attention和全连接的关系？
## reference
- 知乎 一文看懂 Attention（本质原理+3大优点+5大类型） https://zhuanlan.zhihu.com/p/91839581
- 知乎 目前主流的attention方法都有哪些？ https://www.zhihu.com/question/68482809/answer/597944559