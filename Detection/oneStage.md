# oneStage
## SSD
- 创新点
  - 基于FasterRCNN中的anchor，提出了相似的先验框(Prior box)
  - 从不同比例的特征图（多尺度特征）中产生不同比例的预测，并明确地按长宽比分离预测
- 特点
  - 在多个特征图上设置不同缩放比例和不同宽高比的先验框，以融合多尺度特征图进行检测。靠前的大尺度特征图可以捕捉小物体的信息，而靠后的小尺度特征图能捕捉到大物体的信息，从而提高检测的准确性和定位的准确性
- 细节
  - **怎样设置default boxes**
    - 不同于FasterRCNN只在最后一个特征层取anchor，SSD在多个特征曾上取default box，可以得到不同尺度的default box
  - 怎样对先验框进行匹配
    - SSD训练时，只需要输入图像和图像中每个目标对应的ground truth
    - 1) 对图片中的每个ground truth，在先验框中找到与其IOU最大的先验框，则该先验框对应的预测边界框与ground truth匹配
    - 2) 对于1)中上下的没有与任何ground truth匹配到的先验框，找到与其IOU最大的ground truth，若其与该ground truth的IOU值大于某个阈值(一般设为0.5)，则该先验框对应的预测边界框与该ground truth匹配
    - 按照上述两个原则进行匹配，匹配到ground truth的先验框对应的预测边界框为正样本，没有匹配到ground truth的先验框对应的预测边界框为负样本。尽管一个ground truth可以与多个先验框匹配，但是ground truth的数量相对先验框还是很少，按照上述原则进行匹配还是会造成负样本远多于正样本的情况。为了使政府样本尽量均衡(一般保证正负样本比例约为1:3)，SSD采用hard negative mining，即对负样本按照其预测背景类的置信度进行降序排列，选取置信度较小的top-k作为训练的负样本
  - 怎样得到预测的检测结果
    - 分别在所选的特征层上使用3×3卷积核预测不同default boxes所属的类别分数及其预测的边界框，则每个box需要预测c+4个值
    - 对于某个所选的特征层，该层卷积核个数为(c+4)×该层的default box个数
    - 对于得到的每个预测狂，取其类别置信度的最大值，若该最大值大于置信度阈值，则最大值所对应的类别即为该预测框的类别，否则过滤掉此框
    - 对于保留的预测框，根据它对应的先验框进行解码得到其真实的位置参数(这里还要防止预测框位置超出图片)，然后根据所属类别置信度进行降序排列，去top-k个预测框，最后进行NMS，过滤掉重叠度较大的预测框，得到最后检测结果
- 优缺点
  - 优点：快，密集抽样后利用CNN提取特征，直接进行分类与回归
  - 缺点：
    - 均匀密集采样会造成正负样本不均衡，是的训练比较困难，导致模型准确度降低
    - 对小目标的检测没有大目标好，因为随着网络的加深，在高层特征图中小目标的信息丢失掉了，适当增大输入图片的尺寸可以提升小目标的检测效果
## DSSD
- 创新点
  - backbone: 用ResNet-101代替SSD中的VGG，增强了特征提取能力
  - 添加了Deconvolution层，增加了大量上下文信息，以提升小目标检测性能
- 特殊结构
  - Prediction模块
    - 在每个预测层后增加残差模块，提升预测层能力，提高检测精度
  - Deconvolution模块
    - 整合浅层特征图和deconvolution层的信息
    - 在每个卷积层后添加批归一化层(why?)
    - 使用基于学习的deconvolution层而不是简单地双线性上采样
    - 测试了不同的融合方式，发现元素点积比元素求和精度更高
## YOLOv1
- 创新点
  - 将整张图作为网络的输入，直接在输出层回归BBox和类别，创造性地将物体检测任务直接当作回归问题来处理，将候选区和检测两个阶段合二为一
  - 速度快，one stage detection的开山之作
- two stage方法
  - 产生大量可能额包含待检测物体的先验框
  - 分类每个先验框是否包含待检测物体，所属类别的置信度
  - 修正边界框
  - 基于一些准则过滤掉置信度不高和重叠度较高的边界框
- 网络结构
  - 借鉴了GoogLeNet分类网络，使用1×1卷积层和3×3卷积层代替Inception module
  - 包括24个卷积层和2个全连接层
- 输入、输出、损失函数
  - 输入
    - 固定大小（网络最后接了两个全连接层），448×448
    - 将输入图像分成7×7的网格，每个网格预测2个边界框
    - 若某物体ground truth的中心落在该网络，则该网格中与这个ground truth IOU最大的边界框负责预测该物体
  - 输出
    - 7×7×k的张量
    - 每个边界框预测5个值，分别是边界框的中心x,y(相对于所属网格的边界)，边界框的宽高w,h(相对于原始输入图像的宽高的比例)，以及这些边界框的confidence scores(边界框与ground truth box的IOU值)
    - 每个网格还需要预测c个类条件概率即c维向量，表示某个物体在这个网格中，且该object分别属于各个类别的概率，这里的c类物体不包含北京
    - 论文中c=20，则每个网络需要预测2×5+20=30个值
  - 损失函数
    - 坐标预测 + 含有物体的边界框的confidence预测 + 不含物体的边界框的confidence预测 + 分类预测
    - 边界框的confidence = 该边界框存在某类队形的概率pr(object) * 该边界框与该对象的ground truth的IOU值
    - 由于大部分网格中没有物体，对梯度更新的贡献更大，会导致网络不稳定。为了平衡上述问题，YOLO损失函数中对没有物体的边界框的confidence error赋予较小的权重
- 预测流程
- NMS步骤
  - 1) 设定一个score的阈值，一个IOU的阈值
  - 2) 对于每类对象，遍历属于该类的所有候选框
    - 过滤掉score低于阈值的候选框
    - 找到剩下候选框中最大score对应的候选框，添加到输出列表
    - 进一步计算剩下的候选框与上述输出列表中每个候选框的IOU，若该IOU大于设定的IOU阈值，则将该候选框过滤掉，否则加入输出列表中
    - 最后输出列表中的候选框即为图片中该类对象预测的所有边界框
  - 3) 返回步骤2)继续处理下一类对象
- 优缺点
  - 优点
    - YOLO将识别和定位合二为一，结构简单，检测速度快，Fast YOLO可以达到155FPS
    - 相比于RCNN系列，YOLO的整个流程中都能看到整张图像的信息，因此它在检测物体时能很好地利用上下文信息，从而**不容易在背景上预测出从错误的物体信息**
    - YOLO可以学习到高度泛化的特征，能将一个域上学到的特征迁移到不同但相关的域上，如在自然图像上训练的YOLO，在艺术图片上可以得到较好的测试结果
  - 缺点
    - YOLO网格设置比较系数，且每个网格只预测2个边界框，其总体预测精度不高，低于FastRCNN
    - 对小物体的检测效果较差，尤其是对密集的小物体表现比较差
    - 在定位方面不够准确，并且召回率较低
## YOLOv2
- 针对问题：在YOLOv1基础上，提升定位准确率，改善召回率
- 改善方法
  - Batch Normalization
    - 在卷积层后加BN，去掉dropout
    - BN能起到一定的正则化效果，提升模型收敛速度，防止模型过拟合
    - YOLOv2使用BN层，mAP提高了2%
  - High Resolution Classifier
    - 主流分类网络以小于256×*56的图片作为输入进行训练，低分辨率会影响模型检测能力
    - YOLOv2将输入图片的分辨率提升至448×*448，为了使网络适应新的分辨率，YOLOv2先在ImageNet上以448×448的分辨率对网络进行10个epoch的微调，让网络适应高分辨率的输入
    - 通过使用高分辨率的输入，YOLOv2的mAP提升了约4%
  - Convolutional With Anchor Boxes
    - YOLOv1使用全连接层直接对边界框进行预测，导致丢失较多空间信息，定位不准。YOLOv2去掉了YOLOv1中的全连接层。
    - 同时，为了得到更高分辨率的特征图，YOLOv2还去掉了一个池化层
    - 由于图片中的物体都倾向于出现在图片的中心位置，若特征图恰好有一个中心位置，利用这个中心位置预测中心点落入该位置的物体，对这些物体的检测会更容易，所有希望得到的特征图的宽高都为奇数。YOLOv2通过缩减网络，使用416×416的输入，模型下采样总步长为32，最后得到13×13的特征图，然后对每个cell预测5个anchor boxes，每个anchor box预测边界框的位置信息、置信度和一套分类概率值。相比于YOLOv1预测7×7×2个边界框，YOLOv2预测13×13×5个边界框
    - 召回率由81%提升到88%，mAP由69.5%降低到69.2%
  - Demension Clusters
    - FasterRCNN和SSD中，先验框都是手动设定的，带有一定的主观性
    - YOLOv2采用kmeans聚类算法对训练集中的边界框做了聚类分析，选用boxes之间的IOU值作为聚类指标。综合考虑模型复杂度和召回率，最终选择5个聚类中心，得到5个先验框，发现其中扁长的框较少，而瘦高的框更多，更符合行人特征
    - 实验发现，采用聚类分析得到的先验框比手动选择的先验框有更高的平均IOU值，这使得模型更容易训练
  - New Network: Darknet-19
    - 包含19个卷积层和5个maxPooling层，主要采用3×3卷积和1×1卷积
    - 1×1卷积可以压缩特征图通道数以降低模型计算量和参数
    - 每个卷积层后使用BN层以加快模型收敛，同时防止过拟合
    - 最终采用global avg pooling做预测
    - 采用Darknet-19，模型的mAP没有显著提升，但计算量减少了
  - Direct location prediction
    - FasterRCNN使用anchor boxes预测边界框先对先验框的偏移量，由于没有对偏移量进行约束，每个位置预测的边界框可以落在图片任何位置，会导致模型不稳定，增长训练时间
    - YOLOv2沿用YOLOv2的方法，根据所在网格单元的位置来预测坐标，则gt的值位于0-1之间。网络通过将预测结果输入sigmoid，让输出结果介于0-1之间
    - YOLOv2结合Dimension Clusters，通过对边界框的位置预测进行约束，使模型更容易稳定训练，这种方式使得模型的mAP值提升了约5%
  - Fine-Grained Features
    - **YOLOv2借鉴SSD使用多尺度的特征图做预测**，通过pass through层将高分辨率的特征图与低分辨率的特征图联系在一起，从而实现多尺度检测
    - YOLOv2取Darknet-19的最后一个maxPool层的输入，得到26×26×512的特征图。经过1×1×64的卷积以降低特征图的维度，得到26×26×64的特征图，然后经过pass through层的处理变成13×13×256的特征图(抽取原特征图每个2×2的局部区域组成新的channel，即原特征图大小降低4倍，channel增加4倍)，再与13×13×1024大小的特征图拼接，得到13×13×1280的特征图，最后在这些特征图上做预测
    - 使用Fine-Grained Features，YOLOv2的性能提升了1%
  - Multi-Scale Training
    - YOLOv2中使用的Darknet-19网络结果中只有卷积层和池化层，所以其对输入图片的大小没有限制
    - YOLOv2采用多尺度输入的方式训练，在训练过程中每隔10个batches，重新随机选择输入图片的尺寸，由于Darknet-19下采样总步长为32，输入图片的尺寸一般选择32的倍数
    - 采用Multi-Scale Training，可以适应不同大小的图片输入，当采用低分辨率的图片输入时，mAP值略有下降，但速度更快；当采用高分辨率的图片输入时，能耐得到较高mAP值，但速度有所下降
- 优缺点
  - 优点
    - YOLOv2借鉴了很多其它目标检测方法的技巧，如FasterRCNN的anchor boxes，SSD中的多尺度检测
    - YOLOv2在网络设计上做了很多tricks，使它能在保证速度的同时提高检测准确率
    - Multi-Scale Training使得同一个模型适应不同大小的输入，从而可以在速度和精度上进行自由权衡
- 训练
  - 第一阶段：在ImageNet上预训练Darknet-19，此时模型输入为224×224，共训练160个epochs
  - 第二阶段：将网络的输入调整为448×448，继续在ImageNet上finetune，训练10个epochs，此时分类模型的top1准确率为76.5%，而top5准确率为93.3%
  - 第三阶段：修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。网络修改包括：移除最后一个卷积层、global avgPooling层以及softmax层，并新增了3个3×3×2014卷积层，同时增加了一个pass through层，最后使用1×1卷积输出预测结果
## YOLO9000
- 暂且不表
## YOLOv3
- 创新点
  - 新网络结构：DarkNet-53，使用残差模型，进一步加深了网络结构
  - 融合FPN，实现多尺度检测
  - 用逻辑回归代替softmax作为分类器
- 细节
  - 网络结构
    - YOLOv3在Darknet-19的基础上引入了残差模块，进一步加深了网络
    - 改进后的网络有53个卷积层，取名Darknet-53
    - Darknet-53比ResNet-101性能更好，且速度是其1.5倍；Darknet-53与ResNet-152性能相似，速度是其2倍；Darknet-53相比于其它网络结构实现了每秒最高的浮点计算量，说明其网络结构能更好地利用GPU
  - 多尺度检测
    - 相比YOLOv2，YOLOv3提取最后3层特征图，不仅在每个特征图上分别独立做预测，同时通过将小特征图上采样到与大特征图相同大小，然后与大特征图拼接做进一步预测
    - 用维度聚类的思想聚类出9种尺度的anchor box，均匀分配给3种尺度的特征图
- 总结
  - YOLO经历3代变革，在保持速度优势的同时，不断改进网络结构
  - 同时汲取其它优秀目标检测算法的各种trick，先后引入anchor box机制，引入FPN实现多尺度检测
## RetinaNet
- 研究背景
  - Two-Stage检测器(如FasterRCNN、FPN)效果好，但速度相对慢
  - One-Stage检测器(如YOLO、SSD)速度快，但效果一般
  - 作者对one-stage检测器准确率不高的问题进行探究，发现主要问题在于正负样本不均衡(简单-难分类别不均衡)
  - 作者建议通过重新设计交叉熵损失来解决类别不平衡问题，提取Focal loss
  - 问答
    - 什么是类别不均衡(class imbalance)
      - 负样本的数量远大于正样本的数量，比如包含物体的区域(正样本)很少，而不包含物体的区域(负样本)很多。检测算法在早起会生成大量的bbox，而常规图片中一般只有几个object，这意味着，绝大多数bbox输入background
    - 样本的类别不均衡会带来什么问题
      - 由于大多数都是简单易分的负样本(属于背景的样本)，是的训练过程不能充分学习到输入那些有类别样本的信息；其次简单易分的负样本太多，可能掩盖了其它有类别样本的作用(这些简单易分的负样本仍产生一定幅度的loss，数量多会对loss起主要贡献作用，因此就主导了梯度的更新方向，掩盖了重要的信息)
      - 简单来说，bbox中background太多了，所以如果分类器把所有bbox统一归类为background，accuracy也可以很高。于是，分类器的训练就失败了，分类器训练失败，检测精度自然就低了
    - 为什么在two-stage检测器中，没有出现类别不均衡问题
      - 因为通过RPN阶段，可以减少候选目标区域
      - 在分类阶段，可以固定前景与背景比值为1：3，或者使用OHEM(online hard example mining)使得前景和背景的数量达到均衡
- 创新点
  - 新的loss：提出Focal loss解决class imbalance，Focal loss更加聚焦在困难样本(hard examples)上的训练
  - 新的检测器：RetinaNet = ResNet + FPN + Two sub-networks + Focal loss
- Focal loss
  - 交叉熵损失
    - 公式
  - 均衡交叉熵损失：
    - 在loss前添加一个系数控制权重
    - 对于少数类别的样本即正样本，增大系数即可
    - 仅仅解决了正负样本之间的平衡问题，并没有区分难易样本
    - 公式
  - Focal loss
    - 给交叉熵损失函数添加modulating factor，样本越易分，系数越趋近于0
    - 超参数focusing parameter，用于放大modulating factor的作用
    - 既调整正负样本的权重，又控制难易分类样本的权重
    - 公式
- 性能
  - 速度：5FPS
  - 精度：RetinaNet在COCO上mAP为39.1%，可以和FPN及MaskRCNN接近
- RetinaNet Detector
  - Feature Pyramid Network Backbone
  - Classification Subnet
  - BBox Subnet
  - 注意 
    - 训练时FPN每一级的所有example都被用于计算Focal loss，loss值加到一起用来训练
    - 测试时FPN每一级值选取score最大的1000个example来做NMS
    - 整个结构不同层的head部分共享参数，但分类和回归分支间的参数不共享
    - 分类分支最后一级卷积的bias初始化成-log((1-pi)/pi)
## RFBNet
## M2Det
## Remain
- YOLO步骤没太搞懂